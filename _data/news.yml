
- date: 16 July 2025
  headline: "Three papers are accepted by ACM MM 2025, 1) [**SSM for Salient Object Detection**](#), 2) [**ViTCoT**](#) and 3) [**MCM-DPO**](#). Congrats to all my co-authors!"

- date: 26 June 2025
  headline: "Four papers are accepted by ICCV 2025, 1) [**PhysSplat**](#), 2) [**Explainable Driving**](#), 3) [**Derm1M: Clinical Ontology Knowledge**](#) and 4) [**Iris: Self-Refining for GUI Agent**](#). Congrats to all my co-authors!"
  
- date: 20 May 2025
  headline: "We will give a tutorial at [**CVPR 2025**](https://cvpr.thecvf.com/Conferences/2025) on **June 11th**, on the hot topic of [**MLLMs: Evaluations and Benchmarks**](https://mllm2024.github.io/CVPR2025/). Please stay tuned to the [program](https://mllm2024.github.io/CVPR2025/) and welcome on-site or online attendance."

- date: 16 May 2025
  headline: "Three papers are accepted by ACL (Main) 2025, 1) [**Aristotle: Logical Reasoning**](https://arxiv.org/abs/2412.16953), 2) [**Metaphor Detection**](#) and 3) [**Cross-Lingual and Cross-Modal Hallucination Benchmark**](#). Congrats to all my co-authors!"
  
- date: 9 May 2025
  headline: "We are excited to release the project [**On Path to Multimodal Generalist: General-Level and General-Bench**](https://generalist.top/), where we present 1) üöÄ **General-Level**, a novel 5-level evaluation framework for multimodal generalists (multimodal LLMs/agents) by assessing the level of synergy; 2) üçï **General-Bench**, a companion super massive (325K) multimodal benchmark encompasses a broader spectrum of skills, modalities, formats, and capabilities. Welcome submissions to our [**Leaderboards**](https://generalist.top/leaderboard)!"

- date: 1 May 2025
  headline: "Three papers are accepted by ICML 2025, 1) [**Path to Multimodal Generalist (Spotlight!)**](https://arxiv.org/abs/2505.04620), 2) [**VistaDPO: Video-DPO**](https://arxiv.org/abs/2504.13122) and 3) [**Privacy Memorization in MLLM**](https://arxiv.org/abs/2503.01208). Congrats to all my co-authors!"
  
- date: 10 Apr 2025
  headline: "We are holding the grand challenge of [**Multimodal Conversational Aspect-based Sentiment Analysis (PanoSent)**](https://panosent.github.io/MM25-challenge/) and [**Avatar-based Multimodal Empathetic Conversation (AvaMERG)**](https://avamerg.github.io/MM25-challenge/) at ACM Multimedia 2025, **Call for Participation**!"

- date: 5 Apr 2025
  headline: "We are holding the first [**MLLM for Unified Comprehension and Generation (MUCG 2025)**](https://mllm-mucg.github.io/MM2025/) workshop and the first [**Cognition-oriented Multimodal Affective and Empathetic Computing (CogMAEC 2025)**](https://cogmaec.github.io/MM2025/) workshop at ACM Multimedia 2025, **Call for papers**!"

- date: 27 Mar 2025
  headline: "We are holding the first [**Multimodal Knowledge and Language Modeling (MKLM 2025)**](https://sites.google.com/view/ijcai-mklm/home) workshop at IJCAI 2025, **Call for papers**!"

- date: 24 Mar 2025
  headline: "We are releasing the first survey on [**Multimodal Chain-of-Thought Reasoning**](https://arxiv.org/abs/2503.12605), check it now at [**Github**](https://github.com/yaotingwangofficial/Awesome-MCoT)!"
  photo: "<img src='https://pbs.twimg.com/media/Gm34QFpawAAbPRM?format=png&name=4096x4096' class='img-responsive' style='max-width: 800px' />"

- date: 27 Feb 2025
  headline: "Two papers are accepted by CVPR 2025, 1) [**Universal Scene Graph Generation**](https://sqwu.top/USG/) and 2) [**4D Scene Graph Generation**](https://sqwu.top/PSG-4D-LLM/). Congrats to all my co-authors!"
  

- date: 8 Feb 2025
  headline: "One paper about [**Multimodal Grammar Induction**](https://arxiv.org/abs/2410.03739) is accepted by Journal of Artificial Intelligence!"


- date: 22 Jan 2025
  headline: "Two papers are accepted by ICLR 2025, 1) [**Semantic-equivalent Tokenization**](https://arxiv.org/abs/2406.05127) and 2) [**Cross-modal DPO**](#). Congrats to all my co-authors!"
  

- date: 20 Jan 2025
  headline: "Two papers are accepted by TheWebConf (WWW) 2025, 1) [**Multimodal Empathetic Response Generation**](#) and 2) [**Text-based Person Search**](#). Congrats to all my co-authors!"


- date: 1 Jan 2025
  headline: "We are holding the first [**Large Language Models and Structure Modeling (XLLM 2025)**](https://xllms.github.io/) workshop at ACL 2025, **Call for papers** and **Call for Challenge Participation**!"


- date: 10 Dec 2024
  headline: "Five papers are accepted by AAAI 2025, 1) [**MLLM Hallucination**](#), 2) [**Social VQA**](#), 3) [**Multimodal Meme Understanding**](#), 4) [**Chain of Multimodal Thought Benchmark**](#) and 5) [**Intent Detection**](#). Congrats to all my co-authors!"


- date: 2 Nov 2024
  headline: "The tutorial video record of [**Multimodal LLM**](https://mllm2024.github.io/ACM-MM2024/) at [**ACM MM 2024**](https://2024.acmmm.org/) are released at [**Youtube**](https://www.youtube.com/watch?v=hjBGytR9sP4); all slides and materials are available at [**homepage**](https://mllm2024.github.io/ACM-MM2024/)."


- date: 25 Oct 2024
  headline: "We will give a tutorial at [**ACM MM 2024**](https://2024.acmmm.org/) on **Monday 28 Oct 9:00-12:30**, on the hot topic of [**MLLMs: Architecture, Modality, Function, Instruction, Hallucination, Evaluation, Reasoning and Beyond**](https://mllm2024.github.io/ACM-MM2024/). Please stay tuned to the [program](https://mllm2024.github.io/ACM-MM2024/) and welcome on-site or online attendance."

- date: 26 Sep 2024
  headline: "Eight papers are accepted by NeurIPS 2024, all about Multimodal LLMs and Learnings. Congrats to all my co-authors!"

- date: 20 Sep 2024
  headline: "Three papers are accepted by EMNLP 2024 (Main/Findings), 1) [**Commonsense Reasoning**](#), 2) [**Legal Text Generation**](#), and 3) [**Survey on Conversational Understanding**](#). Congrats to all my co-authors!"


- date: 16 Sep 2024
  headline: "Ranked as Top 2% Scientists Worldwide 2024 (Single Year) by Stanford University."


- date: 16 July 2024
  headline: "Four papers are accepted by ACM MM 2024, 1) [**Multimodal Conversational ABSA**](#), 2) [**Speech Event Extraction**](#), 3) [**Mutimodal Coreference Resolution**](#) and 4) [**Visual Programs**](https://arxiv.org/pdf/2311.12890). Congrats to all my co-authors!"


- date: 19 June 2024
  headline: "Our tutorial video of [**Multimodal LLM**](https://mllm2024.github.io/CVPR2024/) at [**CVPR 2024**](https://cvpr.thecvf.com/Conferences/2024/) is released at [**Youtube**](https://www.youtube.com/watch?v=pHBT3zXxQX8); all slides and materials are available at [**homepage**](https://mllm2024.github.io/CVPR2024/)."


- date: 18 June 2024
  headline: "We are excited to receive the World Artificial Intelligence Conference Youth Outstanding Paper Award by our paper, [**NExT-GPT**](https://next-gpt.github.io/)."



- date: 22 May 2024
  headline: "Our tutorial video of [**Multimodal LLM**](https://mllm2024.github.io/COLING2024/) at [**LREC-COLING 2024**](https://lrec-coling-2024.org/) is released at [**Youtube**](https://www.youtube.com/watch?v=-oKpZjaKsAQ); all slides and materials are available at [**homepage**](https://mllm2024.github.io/COLING2024/)."


- date: 16 May 2024
  headline: "Five papers are accepted by ACL 2024 (main or finding), 1) [**Grounded Multimodal UIE**](#), 2) [**Symbolic CoT**](https://arxiv.org/pdf/2405.18357), 3) [**Structured Sentiment Analysis**](#), 4) [**ProtT3: Protein-to-Text Generation**](https://arxiv.org/abs/2405.12564) and 5)[**Conversational Intent Discovery**](#). Congrats to all my co-authors!"

- date: 2 May 2024
  headline: "Three papers are accepted by ICML 2024, 1) [**NExT-GPT**](https://next-gpt.github.io/), 2) [**Video-of-Thought**](http://haofei.vip/VoT/) and 3) [**Video-LLM Momentor**](https://arxiv.org/pdf/2402.11435). Congrats to all my co-authors!"

- date: 25 April 2024
  headline: "One paper about [**Video-Language Modeling**](https://ieeexplore.ieee.org/abstract/document/10508488) is accepted by TPAMI!"

- date: 16 April 2024
  headline: "One paper about [**Few-shot Named Entity Recognition**](https://arxiv.org/pdf/2306.03974.pdf) is accepted by TKDE!"

- date: 15 April 2024
  headline: "We are excited to announce the release of **Vitron** ([**Demo**](https://vitron-llm.github.io/), [**Paper**](http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf), [**Code**](https://github.com/SkyworkAI/Vitron)), a universal pixel-level vision LLM designed for comprehensive understanding (perceiving and reasoning), generating, segmenting (grounding and tracking), editing (inpainting) of both static image and dynamic video content."

- date: 4 March 2024
  headline: "We are holding the Grand Challenge of [__Visual Spatial Description (VSD)__](https://lllogen.github.io/vsd-challenge.github.io/) at [ACM Multimedia 2023](https://www.acmmm2023.org/). Welcome participant!"

- date: 27 Feb 2024
  headline: "Two papers are accepted by CVPR 2024, 1) [**LL3DA**](https://github.com/Open3DA/LL3DA), 2) [**Dysen-VDM**](https://haofei.vip/Dysen-VDM/). Congrats to all my co-authors!"

- date: 20 Feb 2024
  headline: "Our paper **What Factors Influence LLMs‚Äô Judgments? A Case Study on Question Answering** is accepted by LREC-COLING 2024. Congrats to my co-authors!"

- date: 15 Feb 2024
  headline: "We are holding the special session at [ACM ToMM](https://dl.acm.org/journal/tomm) on [__Deep Multimodal Generation and Retrieval__](https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/ACM-SI_ToMM_MMGR-1708635711467.pdf), [Call for submission](https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/ACM-SI_ToMM_MMGR-1708635711467.pdf)."
  
- date: 13 Feb 2024
  headline: "We will give a tutorial at [**CVPR 2024**](https://cvpr.thecvf.com/), on the hot topic of [**From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning and Beyond**](https://mllm2024.github.io/CVPR2024). Please stay tuned to the [program](https://mllm2024.github.io/CVPR2024) and welcome on-site or online attendance."


- date: 1 Feb 2024
  headline: "We will give a tutorial at [**LREC-COLING 2024**](https://lrec-coling-2024.org/), on the hot topic of [**From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning, Efficiency and Beyond**](https://mllm2024.github.io/COLING2024/). Please stay tuned to the [program](https://mllm2024.github.io/COLING2024/) and welcome on-site or online attendance."


- date: 23 Jan 2024
  headline: "Our paper **MMLSCU: A Dataset for Multi-modal Multi-domain Live Streaming Comment Understanding** is accepted by TheWebConf (WWW) 2024. Congrats to my co-authors!"


- date: 14 Dec 2023
  headline: "Our paper **In-Context Learning for Few-Shot Nested Named Entity Recognition** is accepted by ICASSP 2024. Congrats to my co-authors!"


- date: 9 Dec 2023
  headline: "Three papers are accepted by AAAI 2024, 1) **Commonsense Reasoning with Graph-of-Thought**, 2) **DiaASQ**, 3) **Spectral GNN**. Congrats to my co-authors!"


- date: 7 Dec 2023
  headline: "Invited to give a talk at [**CIPS Youth Working Committee in A Star, Singapore**](https://mp.weixin.qq.com/s/LeJwkE_sk29twigo1wxkMw), on the topic of **From Multimodal LLM to AGI**."


- date: 4 Dec 2023
  headline: "[SSNLP 2023](https://wing-nus.github.io/ssnlp-2023/) was held successfully, big thanks to invited speakers and our organizing committee."
  photo: "<img src='/images/newspic/ssnlp23.jpg' class='img-responsive' style='max-width: 800px' />"


- date: 8 Oct 2023
  headline: "One paper is accepted by EMNLP 2023, [**Molca**](https://acharkq.github.io/MolCA/) about **Molecular LLM**, welcome try the [online demo](https://acharkq.github.io/MolCA/) here. Congrats to my co-authors!"


- date: 5 Oct 2023
  headline: "The 2023 Singapore Symposium on Natural Language Processing ([**SSNLP**](https://wing-nus.github.io/ssnlp-2023/#)) will be held on Monday, December 4, at the Shaw Foundation Alumni House Auditorium, NUS. Please [register](https://forms.office.com/r/P7GFnaHHqu)!"


- date: 1 Oct 2023
  headline: "I am excited to announce joining [**Kunlun 2050 Research Lab**](#) as an associate research member."


- date: 30 Sep 2023
  headline: "Two papers are accepted by NeurIPS 2023, 1) [**VPGTrans**](https://vpgtrans.github.io/), 2) [Abstract-to-Intricate **Text-to-Image Synthesis**](https://github.com/ChocoWu/T2I-Salad), Congrats to my co-authors!"


- date: 24 Sep 2023
  headline: "Invited to give a talk at [**MLNLP 2023**](http://mlnlp.world/mlnlp2023/), on the topic of Scene Graph-driven Structured Vision-Language Learning."


- date: 12 Sep 2023
  headline: "We are excited to announce the release of **NExT-GPT** ([**Demo**](https://next-gpt.github.io/), [**Code**](https://github.com/NExT-GPT/NExT-GPT), [**Paper**](https://arxiv.org/pdf/2309.05519)), the first end-to-end MM-LLM that perceives input and generates output in arbitrary combinations (any-to-any) of text, image, video, and audio and beyond."

- date: 25 Aug 2023
  headline: "Invited to give a talk at WING lab @ NUS, on the topic of [**LLM-Empowered Text-to-Vision Diffusion Models**](https://haofei.vip/downloads/20230825-WING-feihao.pdf)"

- date: 23 Aug 2023
  headline: "Invited to give a talk at Institute of Computing Technology, Chinese Academy of Sciences, on the topic of [**Scene Graph-driven Structured Vision-Language Learning**](https://haofei.vip/downloads/20230823-CAS-feihao.pdf)"

- date: 10 Aug 2023
  headline: "Four papers are accepted by ACM MM 2023, about [**Text-to-Image Generation**](https://arxiv.org/pdf/2308.05095.pdf), 
       [**Multimodal Emotion Recognition**](https://arxiv.org/pdf/2308.04502.pdf), 
       [**Video Semantic Role Labeling**](https://arxiv.org/pdf/2308.05081.pdf), 
       and [**Video Moment Retrieval**](#),    
       Congrats to my co-authors!"
       
- date: 4 Aug 2023
  headline: "Our Universal Structured NLP (**XNLP**) demonstration system has been launched online, access it [here](https://xnlp.haofei.vip/), [paper](https://arxiv.org/pdf/2308.01846.pdf)."

- date: 2 Aug 2023
  headline: "I will serve as Senior PC of Speech & Natural Language Processing track at AAAI 2024."
  
- date: 25 July 2023
  headline: "We are holding the special session at [Electronics](https://www.mdpi.com/journal/electronics) on [__Advances in Large Language Model Empowered Machine Learning: Design and Application__](https://www.mdpi.com/journal/electronics/special_issues/RC9S717EB3), [Call for papers](https://www.mdpi.com/journal/electronics/special_issues/RC9S717EB3)."
  
- date: 22 July 2023
  headline: "Our paper [DialogRE^C+: An Extension of DialogRE to Investigate How Much Coreference Helps Relation Extraction in Dialogs](https://arxiv.org/pdf/2308.04498.pdf) has been accepted by NLPCC 2023. Congrats to my collaborators!"

- date: 19 June 2023
  headline: "I am honored to be awarded as the World AI Conference (WAIC'23) Rising Star (2023‰∏ñÁïå‰∫∫Â∑•Êô∫ËÉΩÂ§ß‰ºö‰∫ëÂ∏ÜÂ•ñÊòéÊó•‰πãÊòü)."

- date: 15 June 2023
  headline: "I will serve as Session Chair of Summarization Track (11:00-12:30, 11, July) at [ACL 2023](https://2023.aclweb.org/program/). Meet you there."
  
- date: 18 May 2023
  headline: "I will serve as Senior Area Chair of [EMNLP 2023](https://2023.emnlp.org/)."

- date: 8 May 2023
  headline: "We are releasing a novel work about LLM, [**VPGTrans**: Transfer Visual Prompt Generator across LLMs](https://arxiv.org/abs/2305.01278),
          where we release the **VL-Vicuna**, a multimodal version Vicuna. [Twitter post](https://twitter.com/zhanga6/status/1653619736461340673?s=20),
          [Êú∫Âô®‰πãÂøÉ](https://www.jiqizhixin.com/articles/2023-05-19-11), [Êñ∞Êô∫ÂÖÉ](https://mp.weixin.qq.com/s/7Pf1PCZKdpyWOLdUyBH-Bw).
          Try it out at our [demo page](https://vpgtrans.github.io/)."

- date: 3 May 2023
  headline: "Seven papers are accepted by ACL 2023, Congrats to my co-authors! **Theme-A: Sentiment Analysis** ([Dialogue-level ABSA](https://arxiv.org/pdf/2211.05705.pdf), and [Chain-of-Though Reasoning for Implicit Sentiment](https://arxiv.org/pdf/2305.11255.pdf)), 
          **Theme-B: Information Extraction** ([Multimodal Relation Extraction](https://arxiv.org/pdf/2305.11719.pdf), [Cross-lingual Relation Extraction](https://arxiv.org/pdf/2305.12258.pdf)),
          **Theme-C: Structured Multimodal learning** ([3D Visual Spatial Description](https://arxiv.org/pdf/2305.11768.pdf), [Cross-lingual Captioning](https://arxiv.org/pdf/2305.12260.pdf), [Inference-time Image-free Unsupervised Multimodal Machine Translation](https://arxiv.org/pdf/2305.12256.pdf)).
          See you in Toronto, July."

- date: 29 April 2023
  headline: "We are holding the **Deep Multimodal Learning for Information Retrieval (MMIR 2023)** workshop at ACM Multimedia 2023, [Call for papers](https://videorelation.nextcenter.org/MMIR23/)."

- date: 30 March 2023
  headline: "I will serve as Workshop Co-chair of [EMNLP 2023](https://2023.emnlp.org/)."


- date: 5 March 2023
  headline: "[NSSDM 2023](https://wing-nus.github.io/NSSDM-2023) was held successfully, thanks our keynote speakers;
          also special thanks to our co-organizers [Wenjie](https://wenjiewwj.github.io/), [Yisong](https://yisong.me/) for the hard work!
          See the archive videos of the symposium at [homepage](https://wing-nus.github.io/NSSDM-2023)."
  photo: "<img src='/images/newspic/NSSDM23-panel.jpeg' class='img-responsive' style='max-width: 800px' />"


- date: 3 March 2023
  headline: "Our student volunteers did jobs in serving the [WSDM 2023](https://www.wsdm-conference.org) and made it big success, thank you all!"
  photo: "<img src='/images/newspic/wsdm23-volunteer1.jpg' class='img-responsive' style='max-width: 600px' />
          <img src='/images/newspic/wsdm23-volunteer2.jpg' class='img-responsive' style='max-width: 600px' />"


- date: 20 April 2023
  headline: "Prof. [Min-yen Kan](https://www.comp.nus.edu.sg/~kanmy), [Tat-Seng Chua](https://www.chuatatseng.com/) and I will host the **2023 NUS Symposium on Search and Data Mining (NSSDM 2023)** on **Thursday, 2 Mar** in NUS at [Innovation 4.0](https://maps.google.com/maps?ll=1.294251,103.775931&z=17&t=m&hl=en&gl=SG&mapclient=embed&cid=11892943123017974494).
        We will invite prestigious scholars to give insightful and inspiring keynotes.
        Welcome participant, register at [NSSDM homepage](https://wing-nus.github.io/NSSDM-2023/)."


- date: 15 February 2023
  headline: "Our newly proposed benchmark, **DiaASQ: Conversational Aspect-based Sentiment Quadruple Analysis**, is released on [NLPCC 2023 shared task](http://tcci.ccf.org.cn/conference/2023/cfpt.php).
          Call for participation at [DiaASQ Homepage](https://conasq.pages.dev)."



- date: 8 December  2022
  headline: "I am fortunate to have my Ph.D thesis **On the Syntax-oriented Modeling and Optimization of Natural Language** receive the award of 
      [Excellent Doctoral Thesis of Chinese Information Processing Society of China (CIPSC)](http://www.cipsc.org.cn/chcontent.php?&xuhao=20221122)."



- date: 6 October 2022
  headline: "Two papers are accepted by [EMNLP 2022](https://2022.emnlp.org/), on
          [**Entity-centered Cross-document Relation Extraction**](https://arxiv.org/pdf/2210.16541.pdf), 
          and [**Conversation Disentanglement with Bi-Level Contrastive Learning**](https://arxiv.org/pdf/2210.15265.pdf), 
          Congrats to my co-authors! "


- date: 17 August 2022
  headline: "Two papers are accepted by [COLING 2022](https://coling2022.org/), on
          [**OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction**](https://arxiv.org/pdf/2209.02693.pdf), 
          and [**Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause Pair Extraction**](https://arxiv.org/pdf/2209.04112.pdf), 
          Congrats to Hu Cao and Shunjie! "



- date: 15 June 2022
  headline: "I will serve as Senior Program Committee of [WSDM 2023](https://www.wsdm-conference.org/2023/organizers/senior-program-committee)."


- date: 2 June 2022
  headline: "I will serve as Area Chair of [EMNLP 2022](https://2022.emnlp.org/)."


- date: 30 May 2022
  headline: "I will serve as Student Volunteer Chair of [WSDM 2023](https://www.wsdm-conference.org/2023/organizers/organizing-committee).
        Contact me if you are interested in offering service."


# - date: 1 Jan 2022
#   headline: "See [website viewing statistics](./pageview)."


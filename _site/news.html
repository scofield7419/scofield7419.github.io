<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Hao Fei - News</title>
  <meta name="description" content="Hao Fei - News">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">
  <link rel="canonical" href="http://localhost:4000/news">
<link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000/images/favicon.ico">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>
	
    <a class="navbar-brand" href="http://localhost:4000/"><strong style="color:#05C4B9;">Hao</strong> Fei</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li><a href="http://localhost:4000/research">Research</a></li>
		<li><a href="http://localhost:4000/publications">Publication</a></li>
<!--		<li><a href="http://localhost:4000/projects">Project</a></li>-->
		<li><a href="http://localhost:4000/outputs">Output</a></li>
		<li><a href="http://localhost:4000/services">Service</a></li>
		<li><a href="http://localhost:4000/award">Award</a></li>
		<li><a href="http://localhost:4000/misc">Misc</a></li>
	  </ul>
	</div>
  </div>
</div>



    <div class="container-fluid">
      <div class="row">
        <div id="textid" class="col-sm-12">
  <h1 id="news">News</h1>

<div style="margin-top: 20px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">2 Nov 2024</strong><br /></p>
<p class="text-justify">The tutorial video record of <a href="https://mllm2024.github.io/ACM-MM2024/"><strong>Multimodal LLM</strong></a> at <a href="https://2024.acmmm.org/"><strong>ACM MM 2024</strong></a> are released at <a href="https://www.youtube.com/watch?v=hjBGytR9sP4"><strong>Youtube</strong></a>; all slides and materials are available at <a href="https://mllm2024.github.io/ACM-MM2024/"><strong>homepage</strong></a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">25 Oct 2024</strong><br /></p>
<p class="text-justify">We will give a tutorial at <a href="https://2024.acmmm.org/"><strong>ACM MM 2024</strong></a> on <strong>Monday 28 Oct 9:00-12:30</strong>, on the hot topic of <a href="https://mllm2024.github.io/ACM-MM2024/"><strong>MLLMs: Architecture, Modality, Function, Instruction, Hallucination, Evaluation, Reasoning and Beyond</strong></a>. Please stay tuned to the <a href="https://mllm2024.github.io/ACM-MM2024/">program</a> and welcome on-site or online attendance.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">26 Sep 2024</strong><br /></p>
<p class="text-justify">Eight papers are accepted by NeurIPS 2024, all about Multimodal LLMs and Learnings. Congrats to all my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">20 Sep 2024</strong><br /></p>
<p class="text-justify">Three papers are accepted by EMNLP 2024 (Main/Findings), 1) <a href="#"><strong>Commonsense Reasoning</strong></a>, 2) <a href="#"><strong>Legal Text Generation</strong></a>, and 3) <a href="#"><strong>Survey on Conversational Understanding</strong></a>. Congrats to all my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">16 Sep 2024</strong><br /></p>
<p class="text-justify">Ranked as Top 2% Scientists Worldwide 2024 (Single Year) by Stanford University.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">16 July 2024</strong><br /></p>
<p class="text-justify">Four papers are accepted by ACM MM 2024, 1) <a href="#"><strong>Multimodal Conversational ABSA</strong></a>, 2) <a href="#"><strong>Speech Event Extraction</strong></a>, 3) <a href="#"><strong>Mutimodal Coreference Resolution</strong></a> and 4) <a href="https://arxiv.org/pdf/2311.12890"><strong>Visual Programs</strong></a>. Congrats to all my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">19 June 2024</strong><br /></p>
<p class="text-justify">Our tutorial video of <a href="https://mllm2024.github.io/CVPR2024/"><strong>Multimodal LLM</strong></a> at <a href="https://cvpr.thecvf.com/Conferences/2024/"><strong>CVPR 2024</strong></a> is released at <a href="https://www.youtube.com/watch?v=pHBT3zXxQX8"><strong>Youtube</strong></a>; all slides and materials are available at <a href="https://mllm2024.github.io/CVPR2024/"><strong>homepage</strong></a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">18 June 2024</strong><br /></p>
<p class="text-justify">We are excited to receive the World Artificial Intelligence Conference Youth Outstanding Paper Award by our paper, <a href="https://next-gpt.github.io/"><strong>NExT-GPT</strong></a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">22 May 2024</strong><br /></p>
<p class="text-justify">Our tutorial video of <a href="https://mllm2024.github.io/COLING2024/"><strong>Multimodal LLM</strong></a> at <a href="https://lrec-coling-2024.org/"><strong>LREC-COLING 2024</strong></a> is released at <a href="https://www.youtube.com/watch?v=-oKpZjaKsAQ"><strong>Youtube</strong></a>; all slides and materials are available at <a href="https://mllm2024.github.io/COLING2024/"><strong>homepage</strong></a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">16 May 2024</strong><br /></p>
<p class="text-justify">Five papers are accepted by ACL 2024 (main or finding), 1) <a href="#"><strong>Grounded Multimodal UIE</strong></a>, 2) <a href="https://arxiv.org/pdf/2405.18357"><strong>Symbolic CoT</strong></a>, 3) <a href="#"><strong>Structured Sentiment Analysis</strong></a>, 4) <a href="https://arxiv.org/abs/2405.12564"><strong>ProtT3: Protein-to-Text Generation</strong></a> and 5)<a href="#"><strong>Conversational Intent Discovery</strong></a>. Congrats to all my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">2 May 2024</strong><br /></p>
<p class="text-justify">Three papers are accepted by ICML 2024, 1) <a href="https://next-gpt.github.io/"><strong>NExT-GPT</strong></a>, 2) <a href="http://haofei.vip/VoT/"><strong>Video-of-Thought</strong></a> and 3) <a href="https://arxiv.org/pdf/2402.11435"><strong>Video-LLM Momentor</strong></a>. Congrats to all my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">25 April 2024</strong><br /></p>
<p class="text-justify">One paper about <a href="https://ieeexplore.ieee.org/abstract/document/10508488"><strong>Video-Language Modeling</strong></a> is accepted by TPAMI!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">16 April 2024</strong><br /></p>
<p class="text-justify">One paper about <a href="https://arxiv.org/pdf/2306.03974.pdf"><strong>Few-shot Named Entity Recognition</strong></a> is accepted by TKDE!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">15 April 2024</strong><br /></p>
<p class="text-justify">We are excited to announce the release of <strong>Vitron</strong> (<a href="https://vitron-llm.github.io/"><strong>Demo</strong></a>, <a href="http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf"><strong>Paper</strong></a>, <a href="https://github.com/SkyworkAI/Vitron"><strong>Code</strong></a>), a universal pixel-level vision LLM designed for comprehensive understanding (perceiving and reasoning), generating, segmenting (grounding and tracking), editing (inpainting) of both static image and dynamic video content.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">4 March 2024</strong><br /></p>
<p class="text-justify">We are holding the Grand Challenge of <a href="https://lllogen.github.io/vsd-challenge.github.io/"><strong>Visual Spatial Description (VSD)</strong></a> at <a href="https://www.acmmm2023.org/">ACM Multimedia 2023</a>. Welcome participant!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">27 Feb 2024</strong><br /></p>
<p class="text-justify">Two papers are accepted by CVPR 2024, 1) <a href="https://github.com/Open3DA/LL3DA"><strong>LL3DA</strong></a>, 2) <a href="https://haofei.vip/Dysen-VDM/"><strong>Dysen-VDM</strong></a>. Congrats to all my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">20 Feb 2024</strong><br /></p>
<p class="text-justify">Our paper <strong>What Factors Influence LLMs’ Judgments? A Case Study on Question Answering</strong> is accepted by LREC-COLING 2024. Congrats to my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">15 Feb 2024</strong><br /></p>
<p class="text-justify">We are holding the special session at <a href="https://dl.acm.org/journal/tomm">ACM ToMM</a> on <a href="https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/ACM-SI_ToMM_MMGR-1708635711467.pdf"><strong>Deep Multimodal Generation and Retrieval</strong></a>, <a href="https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/ACM-SI_ToMM_MMGR-1708635711467.pdf">Call for submission</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">13 Feb 2024</strong><br /></p>
<p class="text-justify">We will give a tutorial at <a href="https://cvpr.thecvf.com/"><strong>CVPR 2024</strong></a>, on the hot topic of <a href="https://mllm2024.github.io/CVPR2024"><strong>From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning and Beyond</strong></a>. Please stay tuned to the <a href="https://mllm2024.github.io/CVPR2024">program</a> and welcome on-site or online attendance.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">1 Feb 2024</strong><br /></p>
<p class="text-justify">We will give a tutorial at <a href="https://lrec-coling-2024.org/"><strong>LREC-COLING 2024</strong></a>, on the hot topic of <a href="https://mllm2024.github.io/COLING2024/"><strong>From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning, Efficiency and Beyond</strong></a>. Please stay tuned to the <a href="https://mllm2024.github.io/COLING2024/">program</a> and welcome on-site or online attendance.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">23 Jan 2024</strong><br /></p>
<p class="text-justify">Our paper <strong>MMLSCU: A Dataset for Multi-modal Multi-domain Live Streaming Comment Understanding</strong> is accepted by TheWebConf (WWW) 2024. Congrats to my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">14 Dec 2023</strong><br /></p>
<p class="text-justify">Our paper <strong>In-Context Learning for Few-Shot Nested Named Entity Recognition</strong> is accepted by ICASSP 2024. Congrats to my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">9 Dec 2023</strong><br /></p>
<p class="text-justify">Three papers are accepted by AAAI 2024, 1) <strong>Commonsense Reasoning with Graph-of-Thought</strong>, 2) <strong>DiaASQ</strong>, 3) <strong>Spectral GNN</strong>. Congrats to my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">7 Dec 2023</strong><br /></p>
<p class="text-justify">Invited to give a talk at <a href="https://mp.weixin.qq.com/s/LeJwkE_sk29twigo1wxkMw"><strong>CIPS Youth Working Committee in A Star, Singapore</strong></a>, on the topic of <strong>From Multimodal LLM to AGI</strong>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">4 Dec 2023</strong><br /></p>
<p class="text-justify"><a href="https://wing-nus.github.io/ssnlp-2023/">SSNLP 2023</a> was held successfully, big thanks to invited speakers and our organizing committee.</p>
<p><img src="/images/newspic/ssnlp23.jpg" class="img-responsive" style="max-width: 800px" /></p>
<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">8 Oct 2023</strong><br /></p>
<p class="text-justify">One paper is accepted by EMNLP 2023, <a href="https://acharkq.github.io/MolCA/"><strong>Molca</strong></a> about <strong>Molecular LLM</strong>, welcome try the <a href="https://acharkq.github.io/MolCA/">online demo</a> here. Congrats to my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">5 Oct 2023</strong><br /></p>
<p class="text-justify">The 2023 Singapore Symposium on Natural Language Processing (<a href="https://wing-nus.github.io/ssnlp-2023/#"><strong>SSNLP</strong></a>) will be held on Monday, December 4, at the Shaw Foundation Alumni House Auditorium, NUS. Please <a href="https://forms.office.com/r/P7GFnaHHqu">register</a>!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">1 Oct 2023</strong><br /></p>
<p class="text-justify">I am excited to announce joining <a href="#"><strong>Kunlun 2050 Research Lab</strong></a> as an associate research member.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">30 Sep 2023</strong><br /></p>
<p class="text-justify">Two papers are accepted by NeurIPS 2023, 1) <a href="https://vpgtrans.github.io/"><strong>VPGTrans</strong></a>, 2) <a href="https://github.com/ChocoWu/T2I-Salad">Abstract-to-Intricate <strong>Text-to-Image Synthesis</strong></a>, Congrats to my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">24 Sep 2023</strong><br /></p>
<p class="text-justify">Invited to give a talk at <a href="http://mlnlp.world/mlnlp2023/"><strong>MLNLP 2023</strong></a>, on the topic of Scene Graph-driven Structured Vision-Language Learning.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">12 Sep 2023</strong><br /></p>
<p class="text-justify">We are excited to announce the release of <strong>NExT-GPT</strong> (<a href="https://next-gpt.github.io/"><strong>Demo</strong></a>, <a href="https://github.com/NExT-GPT/NExT-GPT"><strong>Code</strong></a>, <a href="https://arxiv.org/pdf/2309.05519"><strong>Paper</strong></a>), the first end-to-end MM-LLM that perceives input and generates output in arbitrary combinations (any-to-any) of text, image, video, and audio and beyond.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">25 Aug 2023</strong><br /></p>
<p class="text-justify">Invited to give a talk at WING lab @ NUS, on the topic of <a href="https://haofei.vip/downloads/20230825-WING-feihao.pdf"><strong>LLM-Empowered Text-to-Vision Diffusion Models</strong></a></p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">23 Aug 2023</strong><br /></p>
<p class="text-justify">Invited to give a talk at Institute of Computing Technology, Chinese Academy of Sciences, on the topic of <a href="https://haofei.vip/downloads/20230823-CAS-feihao.pdf"><strong>Scene Graph-driven Structured Vision-Language Learning</strong></a></p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">10 Aug 2023</strong><br /></p>
<p class="text-justify">Four papers are accepted by ACM MM 2023, about <a href="https://arxiv.org/pdf/2308.05095.pdf"><strong>Text-to-Image Generation</strong></a>, <a href="https://arxiv.org/pdf/2308.04502.pdf"><strong>Multimodal Emotion Recognition</strong></a>, <a href="https://arxiv.org/pdf/2308.05081.pdf"><strong>Video Semantic Role Labeling</strong></a>, and <a href="#"><strong>Video Moment Retrieval</strong></a>, Congrats to my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">4 Aug 2023</strong><br /></p>
<p class="text-justify">Our Universal Structured NLP (<strong>XNLP</strong>) demonstration system has been launched online, access it <a href="https://xnlp.haofei.vip/">here</a>, <a href="https://arxiv.org/pdf/2308.01846.pdf">paper</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">2 Aug 2023</strong><br /></p>
<p class="text-justify">I will serve as Senior PC of Speech &amp; Natural Language Processing track at AAAI 2024.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">25 July 2023</strong><br /></p>
<p class="text-justify">We are holding the special session at <a href="https://www.mdpi.com/journal/electronics">Electronics</a> on <a href="https://www.mdpi.com/journal/electronics/special_issues/RC9S717EB3"><strong>Advances in Large Language Model Empowered Machine Learning: Design and Application</strong></a>, <a href="https://www.mdpi.com/journal/electronics/special_issues/RC9S717EB3">Call for papers</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">22 July 2023</strong><br /></p>
<p class="text-justify">Our paper <a href="https://arxiv.org/pdf/2308.04498.pdf">DialogRE^C+: An Extension of DialogRE to Investigate How Much Coreference Helps Relation Extraction in Dialogs</a> has been accepted by NLPCC 2023. Congrats to my collaborators!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">19 June 2023</strong><br /></p>
<p class="text-justify">I am honored to be awarded as the World AI Conference (WAIC’23) Rising Star (2023世界人工智能大会云帆奖明日之星).</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">15 June 2023</strong><br /></p>
<p class="text-justify">I will serve as Session Chair of Summarization Track (11:00-12:30, 11, July) at <a href="https://2023.aclweb.org/program/">ACL 2023</a>. Meet you there.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">18 May 2023</strong><br /></p>
<p class="text-justify">I will serve as Senior Area Chair of <a href="https://2023.emnlp.org/">EMNLP 2023</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">8 May 2023</strong><br /></p>
<p class="text-justify">We are releasing a novel work about LLM, <a href="https://arxiv.org/abs/2305.01278"><strong>VPGTrans</strong>: Transfer Visual Prompt Generator across LLMs</a>, where we release the <strong>VL-Vicuna</strong>, a multimodal version Vicuna. <a href="https://twitter.com/zhanga6/status/1653619736461340673?s=20">Twitter post</a>, <a href="https://www.jiqizhixin.com/articles/2023-05-19-11">机器之心</a>, <a href="https://mp.weixin.qq.com/s/7Pf1PCZKdpyWOLdUyBH-Bw">新智元</a>. Try it out at our <a href="https://vpgtrans.github.io/">demo page</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">3 May 2023</strong><br /></p>
<p class="text-justify">Seven papers are accepted by ACL 2023, Congrats to my co-authors! <strong>Theme-A: Sentiment Analysis</strong> (<a href="https://arxiv.org/pdf/2211.05705.pdf">Dialogue-level ABSA</a>, and <a href="https://arxiv.org/pdf/2305.11255.pdf">Chain-of-Though Reasoning for Implicit Sentiment</a>), <strong>Theme-B: Information Extraction</strong> (<a href="https://arxiv.org/pdf/2305.11719.pdf">Multimodal Relation Extraction</a>, <a href="https://arxiv.org/pdf/2305.12258.pdf">Cross-lingual Relation Extraction</a>), <strong>Theme-C: Structured Multimodal learning</strong> (<a href="https://arxiv.org/pdf/2305.11768.pdf">3D Visual Spatial Description</a>, <a href="https://arxiv.org/pdf/2305.12260.pdf">Cross-lingual Captioning</a>, <a href="https://arxiv.org/pdf/2305.12256.pdf">Inference-time Image-free Unsupervised Multimodal Machine Translation</a>). See you in Toronto, July.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">29 April 2023</strong><br /></p>
<p class="text-justify">We are holding the <strong>Deep Multimodal Learning for Information Retrieval (MMIR 2023)</strong> workshop at ACM Multimedia 2023, <a href="https://videorelation.nextcenter.org/MMIR23/">Call for papers</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">30 March 2023</strong><br /></p>
<p class="text-justify">I will serve as Workshop Co-chair of <a href="https://2023.emnlp.org/">EMNLP 2023</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">5 March 2023</strong><br /></p>
<p class="text-justify"><a href="https://wing-nus.github.io/NSSDM-2023">NSSDM 2023</a> was held successfully, thanks our keynote speakers; also special thanks to our co-organizers <a href="https://wenjiewwj.github.io/">Wenjie</a>, <a href="https://yisong.me/">Yisong</a> for the hard work! See the archive videos of the symposium at <a href="https://wing-nus.github.io/NSSDM-2023">homepage</a>.</p>
<p><img src="/images/newspic/NSSDM23-panel.jpeg" class="img-responsive" style="max-width: 800px" /></p>
<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">3 March 2023</strong><br /></p>
<p class="text-justify">Our student volunteers did jobs in serving the <a href="https://www.wsdm-conference.org">WSDM 2023</a> and made it big success, thank you all!</p>
<p><img src="/images/newspic/wsdm23-volunteer1.jpg" class="img-responsive" style="max-width: 600px" /> <img src="/images/newspic/wsdm23-volunteer2.jpg" class="img-responsive" style="max-width: 600px" /></p>
<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">20 April 2023</strong><br /></p>
<p class="text-justify">Prof. <a href="https://www.comp.nus.edu.sg/~kanmy">Min-yen Kan</a>, <a href="https://www.chuatatseng.com/">Tat-Seng Chua</a> and I will host the <strong>2023 NUS Symposium on Search and Data Mining (NSSDM 2023)</strong> on <strong>Thursday, 2 Mar</strong> in NUS at <a href="https://maps.google.com/maps?ll=1.294251,103.775931&amp;z=17&amp;t=m&amp;hl=en&amp;gl=SG&amp;mapclient=embed&amp;cid=11892943123017974494">Innovation 4.0</a>. We will invite prestigious scholars to give insightful and inspiring keynotes. Welcome participant, register at <a href="https://wing-nus.github.io/NSSDM-2023/">NSSDM homepage</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">15 February 2023</strong><br /></p>
<p class="text-justify">Our newly proposed benchmark, <strong>DiaASQ: Conversational Aspect-based Sentiment Quadruple Analysis</strong>, is released on <a href="http://tcci.ccf.org.cn/conference/2023/cfpt.php">NLPCC 2023 shared task</a>. Call for participation at <a href="https://conasq.pages.dev">DiaASQ Homepage</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">8 December  2022</strong><br /></p>
<p class="text-justify">I am fortunate to have my Ph.D thesis <strong>On the Syntax-oriented Modeling and Optimization of Natural Language</strong> receive the award of <a href="http://www.cipsc.org.cn/chcontent.php?&amp;xuhao=20221122">Excellent Doctoral Thesis of Chinese Information Processing Society of China (CIPSC)</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">6 October 2022</strong><br /></p>
<p class="text-justify">Two papers are accepted by <a href="https://2022.emnlp.org/">EMNLP 2022</a>, on <a href="https://arxiv.org/pdf/2210.16541.pdf"><strong>Entity-centered Cross-document Relation Extraction</strong></a>, and <a href="https://arxiv.org/pdf/2210.15265.pdf"><strong>Conversation Disentanglement with Bi-Level Contrastive Learning</strong></a>, Congrats to my co-authors!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">17 August 2022</strong><br /></p>
<p class="text-justify">Two papers are accepted by <a href="https://coling2022.org/">COLING 2022</a>, on <a href="https://arxiv.org/pdf/2209.02693.pdf"><strong>OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction</strong></a>, and <a href="https://arxiv.org/pdf/2209.04112.pdf"><strong>Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause Pair Extraction</strong></a>, Congrats to Hu Cao and Shunjie!</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">15 June 2022</strong><br /></p>
<p class="text-justify">I will serve as Senior Program Committee of <a href="https://www.wsdm-conference.org/2023/organizers/senior-program-committee">WSDM 2023</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">2 June 2022</strong><br /></p>
<p class="text-justify">I will serve as Area Chair of <a href="https://2022.emnlp.org/">EMNLP 2022</a>.</p>

<div style="margin-top: 30px"></div>

<p><span style="font-size: 23px;">•</span>   <strong style="font-size: 18px;">30 May 2022</strong><br /></p>
<p class="text-justify">I will serve as Student Volunteer Chair of <a href="https://www.wsdm-conference.org/2023/organizers/organizing-committee">WSDM 2023</a>. Contact me if you are interested in offering service.</p>

<div style="margin-top: 30px"></div>


</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">

	  <div class="row" style="width: 100%;text-align: center">
		<div class="col-sm-3">
<!--		  <p>&copy2023 Hao Fei. Powered by <a href="http://jekyllrb.com/">Jekyll</a></p>-->
<!--			<p>Powered by <a href="http://jekyllrb.com/">Jekyll</a> </p>-->
<!--		   <p>&copy2023 <a href="http://localhost:4000/"> Hao Fei</a>.</p>-->

		</div>
<!--		<center>-->
		<div class="col-sm-6" >
			  <p style="text-align: center">&copy2024 <a href="http://localhost:4000/"> Hao Fei</a>. &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;
				Powered by <a href="http://jekyllrb.com/">Jekyll</a>.&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;
			  <span id="busuanzi_container_site_pv"><a href=""><span id="busuanzi_value_site_uv"></span></a> visits.</span></p>

<!--			<p>Contact me via email: <a href="mailto:haofei37@nus.edu.sg" target="_blank">haofei37@nus.edu.sg</a></p>-->
		</div>
<!--		</center>-->
		<div class="col-sm-4">
<!--			<span id="busuanzi_container_site_pv">-->
<!--				The site has been visited <span id="busuanzi_value_site_pv"></span> times.-->
<!--				The site visited <a href=""><span id="busuanzi_value_site_pv"></span></a> times.-->
<!--				Visiting <a href=""><span id="busuanzi_value_site_uv"></span></a> times.-->
<!--			</span>-->
<!--			  Coordinates: <br/>-->
<!--				  5 Prince George's Park, Singapore 118404<br/>-->
<!--				  National University of Singapore<br/>-->
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="http://localhost:4000/js/bootstrap.min.js"></script>


  </body>

</html>
